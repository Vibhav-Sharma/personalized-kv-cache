{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RREfMRBxqK1H"
      },
      "outputs": [],
      "source": [
        "# Basic setup\n",
        "!pip install transformers accelerate bitsandbytes datasets\n",
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "!pip install -q faiss-cpu  # Optional if you want to use retrieval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import random\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "arR8wXoArqiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"  # You can later switch to LLaMA or Mistral\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "model.eval()\n",
        "\n"
      ],
      "metadata": {
        "id": "0gzXTNEHsjbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session_contexts = [\n",
        "    \"User: What's the weather like today?\\nAI:\",\n",
        "    \"User: Book me a cab to the airport.\\nAI:\",\n",
        "    \"User: Can you recommend a vegetarian recipe?\\nAI:\",\n",
        "]\n",
        "\n",
        "# Simulate user personalization tokens\n",
        "user_profile = {\"user_id\": 42, \"preferences\": [\"vegetarian\", \"travel\", \"weather\"]}\n"
      ],
      "metadata": {
        "id": "DnV_Yenes7ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KVCacheSimulator:\n",
        "    def __init__(self):\n",
        "        self.cache = []\n",
        "\n",
        "    def add(self, token_id, importance=1.0):\n",
        "        self.cache.append({\"token\": token_id, \"importance\": importance})\n",
        "\n",
        "    def evict_fifo(self):\n",
        "        if self.cache:\n",
        "            self.cache.pop(0)\n",
        "\n",
        "    def evict_least_important(self):\n",
        "        self.cache.sort(key=lambda x: x[\"importance\"])\n",
        "        self.cache.pop(0)\n",
        "\n",
        "    def print_cache(self):\n",
        "        print([token[\"token\"] for token in self.cache])\n"
      ],
      "metadata": {
        "id": "jFZdC5-HtJIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sage_kv_attention_sparsity(attn_weights):\n",
        "    return attn_weights.mean(dim=-1).tolist()  # Simplified\n",
        "\n",
        "def ke_diff_similarity(kv_cache):\n",
        "    similarities = []\n",
        "    for i in range(1, len(kv_cache)):\n",
        "        sim = np.dot(kv_cache[i][\"token\"], kv_cache[i-1][\"token\"])\n",
        "        similarities.append(sim)\n",
        "    return similarities\n"
      ],
      "metadata": {
        "id": "OU5J81E9tR4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = session_contexts[2]\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=30, return_dict_in_generate=True, output_scores=True)\n",
        "generated = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated:\", generated)\n"
      ],
      "metadata": {
        "id": "Sb4XaNantWwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "b1jzOf2gtt96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example cache state\n",
        "cache = [\n",
        "    {\"token\": \"User\", \"importance\": 0.9},\n",
        "    {\"token\": \"vegetarian\", \"importance\": 0.8},\n",
        "    {\"token\": \"airport\", \"importance\": 0.6},\n",
        "    {\"token\": \"recipe\", \"importance\": 0.95},\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df_cache = pd.DataFrame(cache)\n",
        "display(df_cache)\n"
      ],
      "metadata": {
        "id": "yame_UYUty6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KVCacheSimulatorPandas:\n",
        "    def __init__(self):\n",
        "        self.df = pd.DataFrame(columns=[\"token\", \"importance\"])\n",
        "\n",
        "    def add(self, token, importance):\n",
        "        new_row = pd.DataFrame([{\"token\": token, \"importance\": importance}])\n",
        "        self.df = pd.concat([self.df, new_row], ignore_index=True)\n",
        "\n",
        "    def evict_fifo(self):\n",
        "        self.df = self.df.iloc[1:].reset_index(drop=True)\n",
        "\n",
        "    def evict_least_important(self):\n",
        "        idx = self.df[\"importance\"].idxmin()\n",
        "        self.df = self.df.drop(index=idx).reset_index(drop=True)\n",
        "\n",
        "    def show_cache(self):\n",
        "        display(self.df)\n"
      ],
      "metadata": {
        "id": "4ccgcFNQt5JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cache_sim = KVCacheSimulatorPandas()\n",
        "\n",
        "# Adding tokens\n",
        "cache_sim.add(\"User\", 0.9)\n",
        "cache_sim.add(\"vegetarian\", 0.8)\n",
        "cache_sim.add(\"airport\", 0.6)\n",
        "cache_sim.add(\"recipe\", 0.95)\n",
        "\n",
        "# Showing current cache\n",
        "cache_sim.show_cache()\n",
        "\n",
        "# Evicting least important\n",
        "cache_sim.evict_least_important()\n",
        "cache_sim.show_cache()\n"
      ],
      "metadata": {
        "id": "yQF355w-t8lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eviction_log = []\n",
        "\n",
        "def log_eviction(token, reason):\n",
        "    eviction_log.append({\"token\": token, \"reason\": reason})\n",
        "\n",
        "# Log example\n",
        "log_eviction(\"airport\", \"least_important\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_evictions = pd.DataFrame(eviction_log)\n",
        "display(df_evictions)\n"
      ],
      "metadata": {
        "id": "9jtPw90VuDJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  Personalization-Aware KV Cache Eviction\n",
        "This notebook explores techniques for prioritizing and evicting token-level key-value (KV) cache entries in multi-turn LLM inference, with a focus on **user-specific behavior** and **attention-based importance**.\n",
        "\n",
        "The goal is to eventually train a lightweight model that can learn to decide which tokens to retain or evict during long conversations.\n",
        "\n"
      ],
      "metadata": {
        "id": "BIeCvEyyia09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets pandas matplotlib --quiet\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "id": "_0csExEBilpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\", output_attentions=True)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "RmECOjLfi-5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = [\n",
        "    \"Hello, I need help with my math homework.\",\n",
        "    \"Sure! What topic are you struggling with?\",\n",
        "    \"It's about solving quadratic equations.\",\n",
        "    \"Great. Can you give me one example?\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "-Jsek3vVi_u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_data = []\n",
        "\n",
        "for turn_id, sentence in enumerate(session):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        output = model(**inputs)\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "    attn_scores = output.attentions[-1][0][-1].cpu().numpy()  # attention from last layer, last head\n",
        "\n",
        "    for i, token in enumerate(tokens):\n",
        "        token_data.append({\n",
        "            \"turn\": turn_id,\n",
        "            \"token\": token,\n",
        "            \"token_id\": int(inputs['input_ids'][0][i]),\n",
        "            \"attention\": float(attn_scores[i]),\n",
        "        })\n"
      ],
      "metadata": {
        "id": "2wYGxXd2jROG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_data = []\n",
        "\n",
        "for turn_id, sentence in enumerate(session):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        output = model(**inputs)\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "    # ---- attention handling ----\n",
        "    attn = output.attentions[-1][0]                 # (heads, q_len, k_len)\n",
        "    attn_scores = attn.mean(dim=0).mean(dim=0)      # (k_len,)\n",
        "    attn_scores = attn_scores.cpu().numpy()\n",
        "\n",
        "    for i, token in enumerate(tokens):\n",
        "        token_data.append({\n",
        "            \"turn\":       turn_id,\n",
        "            \"token\":      token,\n",
        "            \"token_id\":   int(inputs['input_ids'][0][i]),\n",
        "            \"attention\":  float(attn_scores[i]),\n",
        "        })\n"
      ],
      "metadata": {
        "id": "-h9Tri-ajite"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\", output_attentions=True, attn_implementation=\"eager\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "model.eval()\n",
        "\n",
        "# 2. Session input\n",
        "session = [\n",
        "    \"Hi, how are you?\",\n",
        "    \"Can you suggest me a good movie?\",\n",
        "    \"Thanks!\"\n",
        "]\n",
        "\n",
        "# 3. Get token-level attention\n",
        "token_data = []\n",
        "\n",
        "for turn_id, sentence in enumerate(session):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        output = model(**inputs)\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "    attn = output.attentions[-1][0]                # (heads, q_len, k_len)\n",
        "    attn_scores = attn.mean(dim=0).mean(dim=0)     # (k_len,)\n",
        "    attn_scores = attn_scores.cpu().numpy()\n",
        "\n",
        "    for i, token in enumerate(tokens):\n",
        "        token_data.append({\n",
        "            \"turn\": turn_id,\n",
        "            \"token\": token,\n",
        "            \"token_id\": int(inputs['input_ids'][0][i]),\n",
        "            \"attention\": float(attn_scores[i]),\n",
        "        })\n",
        "\n",
        "# 4. Display as table\n",
        "df = pd.DataFrame(token_data)\n",
        "display(df.head(20))\n"
      ],
      "metadata": {
        "id": "wKVTlAgjjR_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['evict'] = df['attention'] < 0.05  # Example threshold\n",
        "\n"
      ],
      "metadata": {
        "id": "GKw_PQkckAN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['token_id', 'turn', 'attention']]\n",
        "y = df['evict']\n"
      ],
      "metadata": {
        "id": "bhYrs5AMklVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n"
      ],
      "metadata": {
        "id": "gwccgQjGkn5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n"
      ],
      "metadata": {
        "id": "45GH13BlkqYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Predict on the same data (just for a basic check)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# See accuracy\n",
        "print(\"Training Accuracy:\", accuracy_score(y, y_pred))\n"
      ],
      "metadata": {
        "id": "qhc0HLWgk4z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['predicted_evict'] = y_pred\n",
        "df[['token', 'attention', 'evict', 'predicted_evict']].head(10)\n"
      ],
      "metadata": {
        "id": "kFJZE8F6k7f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class TokenImportanceModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(TokenImportanceModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "        self.sigmoid = nn.Sigmoid()  # Output score between 0 and 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        return self.sigmoid(self.fc3(x))\n"
      ],
      "metadata": {
        "id": "aSsG1m7YlBJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***From here starting the tagging system, the main idea***"
      ],
      "metadata": {
        "id": "4TIlI2Cns42K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample prompts and topic labels\n",
        "data = {\n",
        "    \"prompt\": [\n",
        "        \"How does retrieval-augmented generation work?\",\n",
        "        \"Can I eat three chilas at once?\",\n",
        "        \"How to improve aim in Valorant?\",\n",
        "        \"Tell me about KV cache in LLMs\",\n",
        "        \"Give me a diet plan for weight loss\",\n",
        "        \"What is the best crosshair for Vandal?\",\n",
        "        \"Can you help me with my gym routine?\",\n",
        "        \"Difference between BERT and GPT?\",\n",
        "        \"Suggest me good graphics settings for Valorant\",\n",
        "        \"Will skipping dinner help in fat loss?\"\n",
        "    ],\n",
        "    \"label\": [\n",
        "        \"LLM\",\n",
        "        \"Weight loss\",\n",
        "        \"Gaming\",\n",
        "        \"LLM\",\n",
        "        \"Weight loss\",\n",
        "        \"Gaming\",\n",
        "        \"Weight loss\",\n",
        "        \"LLM\",\n",
        "        \"Gaming\",\n",
        "        \"Weight loss\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df\n",
        "\n"
      ],
      "metadata": {
        "id": "uRLaRLC1tD4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode the labels (text to integers)\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "# TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['prompt'])\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df['label_encoded'], test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "KmePhv2Ntjmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# --- Fix: only pass the labels that actually appear in y_test ---\n",
        "unique_test_labels = np.unique(y_test)\n",
        "print(\n",
        "    \"\\nClassification Report:\\n\",\n",
        "    classification_report(\n",
        "        y_test,\n",
        "        y_pred,\n",
        "        labels=unique_test_labels,\n",
        "        target_names=label_encoder.inverse_transform(unique_test_labels),\n",
        "        zero_division=0,   # avoids divideâ€‘byâ€‘zero warnings for small datasets\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "id": "FeuD_qsptnr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_prompt(prompt):\n",
        "    vec = vectorizer.transform([prompt])\n",
        "    pred = model.predict(vec)\n",
        "    return label_encoder.inverse_transform(pred)[0]\n",
        "\n",
        "# Test\n",
        "print(classify_prompt(\"Help me with fine-tuning GPT models\"))\n",
        "print(classify_prompt(\"Can I have cold coffee on keto?\"))\n",
        "print(classify_prompt(\"How to rank up in Valorant?\"))\n"
      ],
      "metadata": {
        "id": "4xVoqVLItwlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_prompts = [\n",
        "    \"What are key-value caches in transformers?\",\n",
        "    \"Explain retrieval-augmented generation with an example.\",\n",
        "    \"How can LLMs adapt to user behavior over time?\",\n",
        "    \"What is SAGE-KV in attention mechanisms?\",\n",
        "    \"How does a sliding window attention work in transformers?\",\n",
        "    \"Which models support dynamic context expansion?\",\n",
        "    \"Compare rotary positional embeddings and absolute embeddings.\",\n",
        "    \"What does attention sparsity mean in transformer models?\",\n",
        "    \"How does fine-tuning differ from instruction tuning?\",\n",
        "    \"Can BART be used with FAISS for retrieval?\",\n",
        "    \"Explain token prioritization in multi-turn LLMs.\",\n",
        "    \"Are KV caches useful in inference optimization?\",\n",
        "    \"What is the benefit of dropout in transformers?\",\n",
        "    \"How do LoRA adapters modify transformer layers?\",\n",
        "    \"Whatâ€™s the best cache eviction strategy for long sessions?\",\n",
        "    \"What is dynamic prompt injection?\",\n",
        "    \"How can user personalization improve chatbot performance?\",\n",
        "    \"What is the role of embeddings in document retrieval?\",\n",
        "    \"How does FAISS work under the hood?\",\n",
        "    \"What are sentence embeddings used for?\",\n",
        "    \"Explain BERT vs SBERT in simple terms.\",\n",
        "    \"Which LLMs support streaming token output?\",\n",
        "    \"What are attention gates used for in LLMs?\",\n",
        "    \"How do we quantify token importance in a KV cache?\",\n",
        "    \"What is token salience ranking?\",\n",
        "    \"Is KV cache storage a bottleneck in LLMs?\",\n",
        "    \"Explain the concept of token-level importance modeling.\",\n",
        "    \"How does OpenAI's GPT handle multi-user memory?\",\n",
        "    \"What are the risks of incorrect KV cache eviction?\",\n",
        "    \"Which metrics are used to evaluate cache-aware LLMs?\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "nL32GGf9uKp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_loss_prompts = [\n",
        "    \"Can I eat 2 chillas at once during diet?\",\n",
        "    \"Is cold coffee okay for weight loss?\",\n",
        "    \"What can I replace jaggery with in tea?\",\n",
        "    \"How many calories in paneer bhurji?\",\n",
        "    \"Can I do intermittent fasting with Indian food?\",\n",
        "    \"Is roti better than rice for weight loss?\",\n",
        "    \"What time should I stop eating at night?\",\n",
        "    \"How to plan a vegetarian low-carb diet?\",\n",
        "    \"Can I drink fruit juice while dieting?\",\n",
        "    \"Is ghee harmful during weight loss?\",\n",
        "    \"Is skipping dinner better than breakfast?\",\n",
        "    \"How often should I eat during IF?\",\n",
        "    \"What is a good weight loss chutney recipe?\",\n",
        "    \"Can I use rock salt in weight loss meals?\",\n",
        "    \"Is poha a good breakfast for dieting?\",\n",
        "    \"Can I drink buttermilk while fasting?\",\n",
        "    \"What are slow carbs and fast carbs?\",\n",
        "    \"Is it okay to have cheat meals?\",\n",
        "    \"How many steps should I walk per day?\",\n",
        "    \"Can I drink flavored water during fast?\",\n",
        "    \"Whatâ€™s a good low-calorie Indian dessert?\",\n",
        "    \"How to reduce bloating during diet?\",\n",
        "    \"Can I eat after 8 PM during weight loss?\",\n",
        "    \"Is coconut water good for weight loss?\",\n",
        "    \"Can I use peanut butter in my diet?\",\n",
        "    \"Should I stop dairy to lose weight?\",\n",
        "    \"What foods cause water retention?\",\n",
        "    \"Best home workouts without equipment?\",\n",
        "    \"Can I add lemon to my green tea?\",\n",
        "    \"Does fasting affect metabolism long-term?\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "bEvy3tWKv0Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valorant_prompts = [\n",
        "    \"How to fix low FPS in Valorant?\",\n",
        "    \"Why is Valorant stuck at 30 FPS?\",\n",
        "    \"Whatâ€™s the best sensitivity for beginners?\",\n",
        "    \"How to check GPU usage while gaming?\",\n",
        "    \"Should I use low latency mode in NVIDIA settings?\",\n",
        "    \"How to enable high performance for Valorant?\",\n",
        "    \"Which is the best agent for ranked solo play?\",\n",
        "    \"How to reduce input lag in Valorant?\",\n",
        "    \"Best crosshair settings for headshots?\",\n",
        "    \"How to use killjoy effectively?\",\n",
        "    \"Can I play Valorant with integrated graphics?\",\n",
        "    \"How to set Valorant on dedicated GPU?\",\n",
        "    \"What are the best system settings for smoother gameplay?\",\n",
        "    \"How to update drivers for Valorant performance?\",\n",
        "    \"Should I cap FPS or leave it unlocked?\",\n",
        "    \"Why does my ping spike in Valorant?\",\n",
        "    \"Can Discord cause FPS drops in Valorant?\",\n",
        "    \"What does 'anti-aliasing' do in Valorant settings?\",\n",
        "    \"Best resolution for competitive Valorant?\",\n",
        "    \"What is 4:3 stretched resolution and is it allowed?\",\n",
        "    \"How to prioritize RAM for gaming?\",\n",
        "    \"Is Valorant CPU or GPU heavy?\",\n",
        "    \"What is the best monitor refresh rate for Valorant?\",\n",
        "    \"How to check if Valorant is using dedicated GPU?\",\n",
        "    \"Does RAM size affect FPS in Valorant?\",\n",
        "    \"How to fix Valorant update errors?\",\n",
        "    \"Can I undervolt CPU for better temps while gaming?\",\n",
        "    \"Does Valorant support dual monitors?\",\n",
        "    \"What is the ideal aim training routine?\",\n",
        "    \"How to record Valorant gameplay without lag?\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "2C-XKeeKv3G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Combine prompts and labels into a DataFrame\n",
        "data = pd.DataFrame({\n",
        "    \"prompt\": llm_prompts + weight_loss_prompts + valorant_prompts,\n",
        "    \"label\": [\"llm\"]*30 + [\"weight_loss\"]*30 + [\"valorant\"]*30\n",
        "})\n",
        "\n",
        "# Shuffle for good measure\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Preview\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "H96W48mYxUx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "data['label_encoded'] = label_encoder.fit_transform(data['label'])\n",
        "\n",
        "# Print mapping\n",
        "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n"
      ],
      "metadata": {
        "id": "1k0OcrSUx5Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(data['prompt'])\n",
        "y = data['label_encoded']\n"
      ],
      "metadata": {
        "id": "j-fj2CrBx_mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "HjykUlJByC9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(max_iter=2000)\n",
        "clf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "YHEQ16YxyGm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Classification Report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "id": "V7Ix-V6-yJTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(clf, X, y, cv=5)\n",
        "print(\"Cross-validation scores:\", scores)\n",
        "print(\"Average:\", scores.mean())\n"
      ],
      "metadata": {
        "id": "irBgTYN8yODC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kbn7zJeOywNp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}